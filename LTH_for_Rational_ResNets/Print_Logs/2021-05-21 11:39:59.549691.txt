Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a539b0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a53cd0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a53d70
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a600f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a604b0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a53dc0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a53eb0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a601e0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a60370
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a605f0
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a60730
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a60f00
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6f0a0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6f6e0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6f960
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a60f50
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6f410
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6f5a0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6f820
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee8a6faa0
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5410
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c52d0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5910
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5b40
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5dc0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c55a0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c53c0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5a00
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5c30
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17c5b90
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17d2140
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17d2b40
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17d2dc0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17dd190
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17dd320
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17d2c30
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17d2640
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17d2be0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17dd0a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17dd460
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17dd5a0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17ddeb0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17eb0f0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17eb870
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17dde10
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17ddf50
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17eb5a0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17eb730
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17eb9b0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17eb5f0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17ebdc0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f55a0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f5af0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f5690
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f5e60
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2fdc510640
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f5780
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f5c30
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17f5f00
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17811e0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17810f0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee1781cd0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee1781f50
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee178d140
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee178d500
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee1781dc0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17817d0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee178d230
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee178d3c0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee178d640
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee178d780
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179a0f0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179a820
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179aaa0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179ad20
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179a1e0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179a2d0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179a960
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179abe0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179ae60
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee179af50
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17a4730
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17a4d70
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17b0140
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17b0320
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17a4820
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17a4910
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17a47d0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17b01e0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f2ee17b0410
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 2.2497 Acc: 0.1761
val Loss: 2.2282 Acc: 0.2170
Epoch finished in 0m 42s
Training Epoch 1/24
********************
Warmup
train Loss: 2.0237 Acc: 0.2824
val Loss: 1.6695 Acc: 0.4281
Epoch finished in 0m 42s
Training Epoch 2/24
********************
Warmup
train Loss: 1.0160 Acc: 0.6671
val Loss: 0.8656 Acc: 0.7475
Epoch finished in 0m 42s
Training Epoch 3/24
********************
Warmup
train Loss: 0.5569 Acc: 0.8241
val Loss: 0.5374 Acc: 0.8424
Epoch finished in 0m 42s
Training Epoch 4/24
********************
Warmup
train Loss: 0.4307 Acc: 0.8671
val Loss: 0.5052 Acc: 0.8478
Epoch finished in 0m 42s
Training Epoch 5/24
********************
Warmup
train Loss: 0.3725 Acc: 0.8869
val Loss: 0.4841 Acc: 0.8593
Epoch finished in 0m 42s
Training Epoch 6/24
********************
Warmup
train Loss: 0.3388 Acc: 0.8981
val Loss: 0.4055 Acc: 0.8854
Epoch finished in 0m 42s
Training Epoch 7/24
********************
Warmup
train Loss: 0.3168 Acc: 0.9052
val Loss: 0.3546 Acc: 0.8918
Epoch finished in 0m 42s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2975 Acc: 0.9103
val Loss: 0.3147 Acc: 0.9047
Epoch finished in 0m 42s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2801 Acc: 0.9162
val Loss: 0.3921 Acc: 0.8827
Epoch finished in 0m 42s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2687 Acc: 0.9193
val Loss: 0.4392 Acc: 0.8798
Epoch finished in 0m 42s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2604 Acc: 0.9227
val Loss: 0.3256 Acc: 0.9071
Epoch finished in 0m 42s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2506 Acc: 0.9265
val Loss: 0.2874 Acc: 0.9135
Epoch finished in 0m 42s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2387 Acc: 0.9297
val Loss: 0.2952 Acc: 0.9167
Epoch finished in 0m 42s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2352 Acc: 0.9320
val Loss: 0.2947 Acc: 0.9142
Epoch finished in 0m 42s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2282 Acc: 0.9336
val Loss: 0.2854 Acc: 0.9119
Epoch finished in 0m 42s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2191 Acc: 0.9367
val Loss: 0.2509 Acc: 0.9273
Epoch finished in 0m 42s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1854 Acc: 0.9463
val Loss: 0.2320 Acc: 0.9315
Epoch finished in 0m 42s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1734 Acc: 0.9503
val Loss: 0.2335 Acc: 0.9317
Epoch finished in 0m 42s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1726 Acc: 0.9504
val Loss: 0.2281 Acc: 0.9349
Epoch finished in 0m 42s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1688 Acc: 0.9525
val Loss: 0.2198 Acc: 0.9364
Epoch finished in 0m 42s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1649 Acc: 0.9534
val Loss: 0.2286 Acc: 0.9333
Epoch finished in 0m 42s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1679 Acc: 0.9525
val Loss: 0.2302 Acc: 0.9333
Epoch finished in 0m 42s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1674 Acc: 0.9519
val Loss: 0.2240 Acc: 0.9354
Epoch finished in 0m 42s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1653 Acc: 0.9526
val Loss: 0.2266 Acc: 0.9352
Epoch finished in 0m 42s
Best validation accuracy: 0.9351861963525172
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.9431468961278425
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  54052.0
Sparsity of Pruned Mask:  tensor(0.2000)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 2.2114 Acc: 0.1974
val Loss: 2.0151 Acc: 0.2945
Epoch finished in 0m 42s
Training Epoch 1/24
********************
Warmup
train Loss: 1.1004 Acc: 0.6369
val Loss: 0.6110 Acc: 0.8116
Epoch finished in 0m 42s
Training Epoch 2/24
********************
Warmup
train Loss: 0.4578 Acc: 0.8601
val Loss: 0.4288 Acc: 0.8722
Epoch finished in 0m 42s
Training Epoch 3/24
********************
Warmup
train Loss: 0.3595 Acc: 0.8919
val Loss: 0.4124 Acc: 0.8851
Epoch finished in 0m 43s
Training Epoch 4/24
********************
Warmup
train Loss: 0.3159 Acc: 0.9051
val Loss: 0.3440 Acc: 0.8944
Epoch finished in 0m 43s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2959 Acc: 0.9117
val Loss: 0.3104 Acc: 0.9071
Epoch finished in 0m 43s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2746 Acc: 0.9179
val Loss: 0.4217 Acc: 0.8856
Epoch finished in 0m 42s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2618 Acc: 0.9223
val Loss: 0.3152 Acc: 0.9132
Epoch finished in 0m 42s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2495 Acc: 0.9248
val Loss: 0.3132 Acc: 0.9155
Epoch finished in 0m 42s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2425 Acc: 0.9291
val Loss: 0.3498 Acc: 0.8961
Epoch finished in 0m 42s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2335 Acc: 0.9313
val Loss: 0.3236 Acc: 0.9082
Epoch finished in 0m 42s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2266 Acc: 0.9321
val Loss: 0.5151 Acc: 0.8941
Epoch finished in 0m 42s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2182 Acc: 0.9353
val Loss: 0.3004 Acc: 0.9109
Epoch finished in 0m 42s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2140 Acc: 0.9365
val Loss: 0.2819 Acc: 0.9179
Epoch finished in 0m 42s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2104 Acc: 0.9386
val Loss: 0.2798 Acc: 0.9227
Epoch finished in 0m 42s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2068 Acc: 0.9392
val Loss: 0.2742 Acc: 0.9217
Epoch finished in 0m 42s
Training Epoch 16/24
********************
Warmup
train Loss: 0.1988 Acc: 0.9426
val Loss: 0.2550 Acc: 0.9293
Epoch finished in 0m 42s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1683 Acc: 0.9512
val Loss: 0.2382 Acc: 0.9316
Epoch finished in 0m 42s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1580 Acc: 0.9543
val Loss: 0.2266 Acc: 0.9351
Epoch finished in 0m 42s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1512 Acc: 0.9554
val Loss: 0.2251 Acc: 0.9351
Epoch finished in 0m 42s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1503 Acc: 0.9561
val Loss: 0.2240 Acc: 0.9368
Epoch finished in 0m 42s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1498 Acc: 0.9574
val Loss: 0.2238 Acc: 0.9361
Epoch finished in 0m 42s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1480 Acc: 0.9578
val Loss: 0.2201 Acc: 0.9376
Epoch finished in 0m 42s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1497 Acc: 0.9577
val Loss: 0.2231 Acc: 0.9356
Epoch finished in 0m 42s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1472 Acc: 0.9564
val Loss: 0.2270 Acc: 0.9335
Epoch finished in 0m 42s
Best validation accuracy: 0.9334935022387245
Model Test Accuracy:  0.9483712354025814
Pruning Epoch 2
++++++++++++++++++
number of weights to prune:  43241.0
Sparsity of Pruned Mask:  tensor(0.3600)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.5186 Acc: 0.4919
val Loss: 0.7336 Acc: 0.7949
Epoch finished in 0m 42s
Training Epoch 1/24
********************
Warmup
train Loss: 0.4204 Acc: 0.8715
val Loss: 0.4970 Acc: 0.8518
Epoch finished in 0m 42s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3214 Acc: 0.9050
val Loss: 0.3823 Acc: 0.9005
Epoch finished in 0m 42s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2793 Acc: 0.9167
val Loss: 0.4447 Acc: 0.8906
Epoch finished in 0m 42s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2565 Acc: 0.9251
val Loss: 0.3272 Acc: 0.9070
Epoch finished in 0m 42s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2443 Acc: 0.9288
val Loss: 0.2943 Acc: 0.9154
Epoch finished in 0m 42s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2305 Acc: 0.9315
val Loss: 0.3091 Acc: 0.9099
Epoch finished in 0m 43s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2250 Acc: 0.9332
val Loss: 0.3216 Acc: 0.9104
Epoch finished in 0m 43s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2194 Acc: 0.9362
val Loss: 0.3158 Acc: 0.9144
Epoch finished in 0m 43s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2119 Acc: 0.9379
val Loss: 0.2865 Acc: 0.9154
Epoch finished in 0m 43s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2059 Acc: 0.9404
val Loss: 0.2673 Acc: 0.9272
Epoch finished in 0m 42s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2033 Acc: 0.9401
val Loss: 0.2663 Acc: 0.9238
Epoch finished in 0m 42s
Training Epoch 12/24
********************
Warmup
train Loss: 0.1984 Acc: 0.9423
val Loss: 0.2696 Acc: 0.9216
Epoch finished in 0m 42s
Training Epoch 13/24
********************
Warmup
train Loss: 0.1981 Acc: 0.9420
val Loss: 0.2897 Acc: 0.9239
Epoch finished in 0m 42s
Training Epoch 14/24
********************
Warmup
train Loss: 0.1920 Acc: 0.9428
val Loss: 0.2589 Acc: 0.9266
Epoch finished in 0m 42s
Training Epoch 15/24
********************
Warmup
train Loss: 0.1893 Acc: 0.9437
val Loss: 0.2990 Acc: 0.9211
Epoch finished in 0m 42s
Training Epoch 16/24
********************
Warmup
train Loss: 0.1841 Acc: 0.9463
val Loss: 0.2341 Acc: 0.9319
Epoch finished in 0m 42s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1579 Acc: 0.9545
val Loss: 0.2287 Acc: 0.9350
Epoch finished in 0m 42s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1470 Acc: 0.9579
val Loss: 0.2282 Acc: 0.9370
Epoch finished in 0m 42s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1421 Acc: 0.9591
val Loss: 0.2263 Acc: 0.9368
Epoch finished in 0m 42s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1389 Acc: 0.9604
val Loss: 0.2199 Acc: 0.9384
Epoch finished in 0m 42s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1389 Acc: 0.9607
val Loss: 0.2262 Acc: 0.9375
Epoch finished in 0m 42s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1387 Acc: 0.9595
val Loss: 0.2207 Acc: 0.9380
Epoch finished in 0m 42s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1402 Acc: 0.9601
val Loss: 0.2234 Acc: 0.9369
Epoch finished in 0m 42s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1387 Acc: 0.9602
val Loss: 0.2199 Acc: 0.9373
Epoch finished in 0m 42s
Best validation accuracy: 0.9373157147537403
Model Test Accuracy:  0.9459511370620773
Pruning Epoch 3
++++++++++++++++++
number of weights to prune:  34593.0
Sparsity of Pruned Mask:  tensor(0.4880)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.5744 Acc: 0.4956
val Loss: 0.7029 Acc: 0.7872
Epoch finished in 0m 42s
Training Epoch 1/24
********************
Warmup
train Loss: 0.4020 Acc: 0.8767
val Loss: 0.3766 Acc: 0.8869
Epoch finished in 0m 42s
Training Epoch 2/24
********************
Warmup
train Loss: 0.2934 Acc: 0.9123
val Loss: 0.4794 Acc: 0.9020
Epoch finished in 0m 42s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2512 Acc: 0.9245
val Loss: 0.2814 Acc: 0.9148
Epoch finished in 0m 42s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2323 Acc: 0.9311
val Loss: 0.3006 Acc: 0.9107
Epoch finished in 0m 42s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2186 Acc: 0.9356
val Loss: 0.2743 Acc: 0.9218
Epoch finished in 0m 42s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2115 Acc: 0.9370
val Loss: 0.2647 Acc: 0.9234
Epoch finished in 0m 42s
Training Epoch 7/24
********************
