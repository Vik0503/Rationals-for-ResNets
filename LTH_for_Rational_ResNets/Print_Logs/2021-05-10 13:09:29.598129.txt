Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39a7910
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39b3c80
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6af363b320
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39bd550
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39bda00
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39bde60
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39cb370
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39cb7d0
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac39cbc30
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac2755320
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac27557d0
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac2755c30
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac2755fa0
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac275e5a0
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac275ea50
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac275efa0
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac276e5a0
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f6ac276ea00
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac277b2d0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac277beb0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2788500
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2788410
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27883c0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac277bf00
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac277bfa0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2788050
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2788640
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2788550
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27887d0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711460
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27114b0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27113c0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711af0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711410
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711370
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27112d0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711c30
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711fa0
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2711eb0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2720960
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27209b0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2720e60
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272c0f0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2720910
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2720870
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27208c0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272c370
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272c280
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272c4b0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272cc30
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272c730
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac273a0a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac273a410
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272ccd0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac272cd70
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac273a690
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac273a550
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac273a460
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac273a6e0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2743640
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2743690
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2743e60
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2743c30
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27435f0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2743550
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27434b0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2743d70
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26d03c0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26d0190
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26d0140
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26d0af0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26dc190
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26dc550
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26d0be0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26d0c80
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26dc410
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26dc320
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26dc2d0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26dc460
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e8280
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e82d0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e81e0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e88c0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e8230
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e8190
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e80f0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e8780
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e8a00
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26e8c80
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26f29b0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26f2820
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2700370
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2700280
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26f2960
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26f28c0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac26f2910
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27000f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac27004b0
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2700230
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2700640
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac270b640
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac270b690
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac270b500
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2700f00
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac2700f50
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac270b140
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac270b0a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f6ac270b7d0
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 2.2523 Acc: 0.1853
val Loss: 2.2186 Acc: 0.1960
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 2.0473 Acc: 0.2724
val Loss: 1.9013 Acc: 0.3202
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 1.6277 Acc: 0.4282
val Loss: 1.3510 Acc: 0.5304
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 1.0261 Acc: 0.6591
val Loss: 0.7533 Acc: 0.7507
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.6156 Acc: 0.8026
val Loss: 0.5651 Acc: 0.8185
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.4857 Acc: 0.8476
val Loss: 0.4578 Acc: 0.8564
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.4167 Acc: 0.8703
val Loss: 0.4530 Acc: 0.8566
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.3777 Acc: 0.8833
val Loss: 0.3920 Acc: 0.8773
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.3504 Acc: 0.8921
val Loss: 0.3927 Acc: 0.8773
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.3333 Acc: 0.8986
val Loss: 0.3844 Acc: 0.8796
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.3163 Acc: 0.9042
val Loss: 0.3563 Acc: 0.8893
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.3025 Acc: 0.9097
val Loss: 0.3028 Acc: 0.9095
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2947 Acc: 0.9099
val Loss: 0.3276 Acc: 0.8993
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2838 Acc: 0.9138
val Loss: 0.3087 Acc: 0.9065
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2757 Acc: 0.9159
val Loss: 0.2943 Acc: 0.9114
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2695 Acc: 0.9192
val Loss: 0.3347 Acc: 0.8987
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2646 Acc: 0.9215
val Loss: 0.2638 Acc: 0.9188
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.2289 Acc: 0.9324
val Loss: 0.2412 Acc: 0.9294
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.2197 Acc: 0.9346
val Loss: 0.2311 Acc: 0.9314
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.2137 Acc: 0.9372
val Loss: 0.2320 Acc: 0.9325
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.2096 Acc: 0.9386
val Loss: 0.2305 Acc: 0.9325
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.2109 Acc: 0.9381
val Loss: 0.2322 Acc: 0.9303
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.2094 Acc: 0.9385
val Loss: 0.2313 Acc: 0.9333
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.2076 Acc: 0.9399
val Loss: 0.2325 Acc: 0.9316
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.2080 Acc: 0.9390
val Loss: 0.2330 Acc: 0.9315
Epoch finished in 0m 9s
Best validation accuracy: 0.9315277929452878
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.9388829133374308
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  53540.0
Sparsity of Pruned Mask:  tensor(0.2000)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 2.1414 Acc: 0.2427
val Loss: 1.9320 Acc: 0.3136
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 1.5813 Acc: 0.4565
val Loss: 1.1926 Acc: 0.5917
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.7670 Acc: 0.7612
val Loss: 0.6000 Acc: 0.8042
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.4877 Acc: 0.8471
val Loss: 0.4443 Acc: 0.8638
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.4000 Acc: 0.8761
val Loss: 0.4457 Acc: 0.8587
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.3626 Acc: 0.8884
val Loss: 0.3558 Acc: 0.8931
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.3331 Acc: 0.8974
val Loss: 0.3597 Acc: 0.8903
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.3190 Acc: 0.9016
val Loss: 0.3180 Acc: 0.9035
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.3071 Acc: 0.9064
val Loss: 0.3495 Acc: 0.8927
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2949 Acc: 0.9106
val Loss: 0.3408 Acc: 0.8959
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2858 Acc: 0.9129
val Loss: 0.2975 Acc: 0.9110
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2751 Acc: 0.9163
val Loss: 0.3292 Acc: 0.9039
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2703 Acc: 0.9169
val Loss: 0.3793 Acc: 0.8825
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2613 Acc: 0.9213
val Loss: 0.2751 Acc: 0.9186
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2564 Acc: 0.9229
val Loss: 0.3148 Acc: 0.9047
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2512 Acc: 0.9249
val Loss: 0.2819 Acc: 0.9149
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2485 Acc: 0.9252
val Loss: 0.2495 Acc: 0.9259
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.2150 Acc: 0.9367
val Loss: 0.2315 Acc: 0.9310
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.2054 Acc: 0.9398
val Loss: 0.2307 Acc: 0.9311
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1990 Acc: 0.9414
val Loss: 0.2248 Acc: 0.9344
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1954 Acc: 0.9426
val Loss: 0.2257 Acc: 0.9346
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1954 Acc: 0.9434
val Loss: 0.2244 Acc: 0.9344
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1971 Acc: 0.9427
val Loss: 0.2228 Acc: 0.9346
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1952 Acc: 0.9440
val Loss: 0.2223 Acc: 0.9345
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1969 Acc: 0.9423
val Loss: 0.2256 Acc: 0.9331
Epoch finished in 0m 9s
Best validation accuracy: 0.933111280987223
Model Test Accuracy:  0.9424170251997541
Pruning Epoch 2
++++++++++++++++++
number of weights to prune:  42831.0
Sparsity of Pruned Mask:  tensor(0.3600)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 2.0044 Acc: 0.3053
val Loss: 1.6057 Acc: 0.4811
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 1.0060 Acc: 0.6981
val Loss: 0.5734 Acc: 0.8260
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.4692 Acc: 0.8581
val Loss: 0.3915 Acc: 0.8830
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.3658 Acc: 0.8886
val Loss: 0.3668 Acc: 0.8882
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.3276 Acc: 0.9009
val Loss: 0.3388 Acc: 0.8976
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.3049 Acc: 0.9084
val Loss: 0.3302 Acc: 0.9008
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2906 Acc: 0.9119
val Loss: 0.3086 Acc: 0.9068
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2789 Acc: 0.9167
val Loss: 0.2957 Acc: 0.9112
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2707 Acc: 0.9180
val Loss: 0.3262 Acc: 0.9009
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2674 Acc: 0.9188
val Loss: 0.2893 Acc: 0.9128
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2594 Acc: 0.9212
val Loss: 0.3082 Acc: 0.9059
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2520 Acc: 0.9234
val Loss: 0.2800 Acc: 0.9167
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2512 Acc: 0.9246
val Loss: 0.3034 Acc: 0.9067
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2443 Acc: 0.9268
val Loss: 0.2834 Acc: 0.9136
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2384 Acc: 0.9287
val Loss: 0.2894 Acc: 0.9139
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2370 Acc: 0.9287
val Loss: 0.3014 Acc: 0.9104
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2301 Acc: 0.9308
val Loss: 0.2446 Acc: 0.9268
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1986 Acc: 0.9410
val Loss: 0.2245 Acc: 0.9340
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1909 Acc: 0.9446
val Loss: 0.2199 Acc: 0.9367
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1869 Acc: 0.9453
val Loss: 0.2225 Acc: 0.9352
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1824 Acc: 0.9473
val Loss: 0.2155 Acc: 0.9390
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1825 Acc: 0.9472
val Loss: 0.2160 Acc: 0.9370
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1835 Acc: 0.9469
val Loss: 0.2176 Acc: 0.9363
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1843 Acc: 0.9469
val Loss: 0.2182 Acc: 0.9376
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1823 Acc: 0.9460
val Loss: 0.2180 Acc: 0.9375
Epoch finished in 0m 9s
Best validation accuracy: 0.9375341268974555
Model Test Accuracy:  0.9468346650276582
Pruning Epoch 3
++++++++++++++++++
number of weights to prune:  34265.0
Sparsity of Pruned Mask:  tensor(0.4880)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.8414 Acc: 0.3918
val Loss: 1.1829 Acc: 0.6850
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.6826 Acc: 0.8191
val Loss: 0.4361 Acc: 0.8750
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3839 Acc: 0.8867
val Loss: 0.3373 Acc: 0.8981
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.3126 Acc: 0.9056
val Loss: 0.3095 Acc: 0.9068
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2915 Acc: 0.9105
val Loss: 0.2976 Acc: 0.9091
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2727 Acc: 0.9170
val Loss: 0.3013 Acc: 0.9072
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2638 Acc: 0.9203
val Loss: 0.2808 Acc: 0.9170
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2537 Acc: 0.9238
val Loss: 0.3150 Acc: 0.9048
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2490 Acc: 0.9247
val Loss: 0.2815 Acc: 0.9154
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2429 Acc: 0.9273
val Loss: 0.2751 Acc: 0.9174
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2418 Acc: 0.9285
val Loss: 0.2916 Acc: 0.9136
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2371 Acc: 0.9285
val Loss: 0.2823 Acc: 0.9153
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2309 Acc: 0.9302
val Loss: 0.2854 Acc: 0.9161
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2335 Acc: 0.9295
val Loss: 0.2746 Acc: 0.9172
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2314 Acc: 0.9308
val Loss: 0.2950 Acc: 0.9123
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2267 Acc: 0.9325
val Loss: 0.2755 Acc: 0.9182
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2191 Acc: 0.9352
val Loss: 0.2374 Acc: 0.9290
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1953 Acc: 0.9422
val Loss: 0.2224 Acc: 0.9342
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1831 Acc: 0.9465
val Loss: 0.2143 Acc: 0.9368
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1780 Acc: 0.9483
val Loss: 0.2161 Acc: 0.9374
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1756 Acc: 0.9493
val Loss: 0.2131 Acc: 0.9388
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1771 Acc: 0.9483
val Loss: 0.2105 Acc: 0.9387
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1746 Acc: 0.9491
val Loss: 0.2126 Acc: 0.9396
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1762 Acc: 0.9501
val Loss: 0.2109 Acc: 0.9397
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1749 Acc: 0.9495
val Loss: 0.2110 Acc: 0.9380
Epoch finished in 0m 9s
Best validation accuracy: 0.9379709511848859
Model Test Accuracy:  0.946220036877689
Pruning Epoch 4
++++++++++++++++++
number of weights to prune:  27412.0
Sparsity of Pruned Mask:  tensor(0.5904)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.7239 Acc: 0.4583
val Loss: 0.9709 Acc: 0.7638
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5721 Acc: 0.8533
val Loss: 0.3938 Acc: 0.8890
Epoch finished in 0m 10s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3476 Acc: 0.8992
val Loss: 0.3179 Acc: 0.9060
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2907 Acc: 0.9132
val Loss: 0.3413 Acc: 0.8969
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2671 Acc: 0.9203
val Loss: 0.2815 Acc: 0.9144
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2572 Acc: 0.9232
val Loss: 0.2923 Acc: 0.9111
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2473 Acc: 0.9248
val Loss: 0.2638 Acc: 0.9200
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2391 Acc: 0.9284
val Loss: 0.2659 Acc: 0.9184
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2368 Acc: 0.9287
val Loss: 0.2768 Acc: 0.9153
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2334 Acc: 0.9287
val Loss: 0.2740 Acc: 0.9192
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2294 Acc: 0.9314
val Loss: 0.2631 Acc: 0.9207
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2265 Acc: 0.9321
val Loss: 0.2610 Acc: 0.9230
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2217 Acc: 0.9344
val Loss: 0.2799 Acc: 0.9152
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2195 Acc: 0.9340
val Loss: 0.2671 Acc: 0.9216
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2207 Acc: 0.9338
val Loss: 0.2706 Acc: 0.9203
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2183 Acc: 0.9346
val Loss: 0.2754 Acc: 0.9196
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2191 Acc: 0.9352
val Loss: 0.2357 Acc: 0.9296
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1923 Acc: 0.9440
val Loss: 0.2213 Acc: 0.9360
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1779 Acc: 0.9482
val Loss: 0.2158 Acc: 0.9363
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1741 Acc: 0.9502
val Loss: 0.2124 Acc: 0.9376
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1706 Acc: 0.9508
val Loss: 0.2178 Acc: 0.9370
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1721 Acc: 0.9504
val Loss: 0.2093 Acc: 0.9402
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1720 Acc: 0.9511
val Loss: 0.2136 Acc: 0.9372
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1710 Acc: 0.9510
val Loss: 0.2162 Acc: 0.9372
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1702 Acc: 0.9514
val Loss: 0.2129 Acc: 0.9390
Epoch finished in 0m 9s
Best validation accuracy: 0.939008408867533
Model Test Accuracy:  0.9462968653964351
Pruning Epoch 5
++++++++++++++++++
number of weights to prune:  21929.0
Sparsity of Pruned Mask:  tensor(0.6723)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.6485 Acc: 0.5082
val Loss: 0.8297 Acc: 0.8177
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5262 Acc: 0.8674
val Loss: 0.3774 Acc: 0.8928
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3267 Acc: 0.9050
val Loss: 0.3062 Acc: 0.9090
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2776 Acc: 0.9165
val Loss: 0.2890 Acc: 0.9145
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2559 Acc: 0.9231
val Loss: 0.2762 Acc: 0.9168
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2456 Acc: 0.9263
val Loss: 0.2821 Acc: 0.9141
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2344 Acc: 0.9307
val Loss: 0.2748 Acc: 0.9164
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2332 Acc: 0.9301
val Loss: 0.2699 Acc: 0.9215
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2263 Acc: 0.9333
val Loss: 0.2634 Acc: 0.9203
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2231 Acc: 0.9328
val Loss: 0.2638 Acc: 0.9211
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2210 Acc: 0.9339
val Loss: 0.2584 Acc: 0.9250
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2215 Acc: 0.9338
val Loss: 0.2938 Acc: 0.9090
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2157 Acc: 0.9357
val Loss: 0.2559 Acc: 0.9245
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2169 Acc: 0.9356
val Loss: 0.2666 Acc: 0.9224
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2140 Acc: 0.9370
val Loss: 0.2889 Acc: 0.9149
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2145 Acc: 0.9357
val Loss: 0.2527 Acc: 0.9260
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2110 Acc: 0.9364
val Loss: 0.2313 Acc: 0.9338
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1852 Acc: 0.9463
val Loss: 0.2174 Acc: 0.9373
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1752 Acc: 0.9498
val Loss: 0.2144 Acc: 0.9371
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1708 Acc: 0.9507
val Loss: 0.2079 Acc: 0.9398
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1683 Acc: 0.9516
val Loss: 0.2139 Acc: 0.9385
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1696 Acc: 0.9505
val Loss: 0.2107 Acc: 0.9395
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1678 Acc: 0.9515
val Loss: 0.2083 Acc: 0.9402
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1681 Acc: 0.9516
val Loss: 0.2097 Acc: 0.9387
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1669 Acc: 0.9526
val Loss: 0.2115 Acc: 0.9391
Epoch finished in 0m 9s
Best validation accuracy: 0.9391176149393906
Model Test Accuracy:  0.9441840811309157
Pruning Epoch 6
++++++++++++++++++
number of weights to prune:  17543.0
Sparsity of Pruned Mask:  tensor(0.7379)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.6422 Acc: 0.5086
val Loss: 0.8162 Acc: 0.8199
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5120 Acc: 0.8731
val Loss: 0.3790 Acc: 0.8944
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3151 Acc: 0.9092
val Loss: 0.3074 Acc: 0.9056
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2694 Acc: 0.9193
val Loss: 0.2730 Acc: 0.9195
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2479 Acc: 0.9264
val Loss: 0.3072 Acc: 0.9059
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2356 Acc: 0.9297
val Loss: 0.2651 Acc: 0.9216
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2326 Acc: 0.9296
val Loss: 0.2500 Acc: 0.9245
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2241 Acc: 0.9332
val Loss: 0.2560 Acc: 0.9250
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2219 Acc: 0.9333
val Loss: 0.2722 Acc: 0.9191
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2160 Acc: 0.9360
val Loss: 0.2554 Acc: 0.9246
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2209 Acc: 0.9336
val Loss: 0.2492 Acc: 0.9279
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2143 Acc: 0.9359
val Loss: 0.2623 Acc: 0.9218
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2125 Acc: 0.9365
val Loss: 0.2624 Acc: 0.9239
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2121 Acc: 0.9363
val Loss: 0.2701 Acc: 0.9197
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2124 Acc: 0.9357
val Loss: 0.2489 Acc: 0.9265
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2091 Acc: 0.9368
val Loss: 0.2481 Acc: 0.9273
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2056 Acc: 0.9380
val Loss: 0.2229 Acc: 0.9356
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1768 Acc: 0.9491
val Loss: 0.2172 Acc: 0.9378
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1683 Acc: 0.9510
val Loss: 0.2111 Acc: 0.9393
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1672 Acc: 0.9515
val Loss: 0.2081 Acc: 0.9391
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1642 Acc: 0.9534
val Loss: 0.2115 Acc: 0.9394
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1634 Acc: 0.9528
val Loss: 0.2158 Acc: 0.9372
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1658 Acc: 0.9526
val Loss: 0.2079 Acc: 0.9396
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1637 Acc: 0.9528
val Loss: 0.2105 Acc: 0.9379
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1638 Acc: 0.9531
val Loss: 0.2085 Acc: 0.9392
Epoch finished in 0m 9s
Best validation accuracy: 0.9391722179753195
Model Test Accuracy:  0.9474877074370005
Pruning Epoch 7
++++++++++++++++++
number of weights to prune:  14034.0
Sparsity of Pruned Mask:  tensor(0.7903)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.6305 Acc: 0.5181
val Loss: 0.8285 Acc: 0.8247
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5063 Acc: 0.8774
val Loss: 0.3660 Acc: 0.8958
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3111 Acc: 0.9096
val Loss: 0.2879 Acc: 0.9162
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2611 Acc: 0.9223
val Loss: 0.2746 Acc: 0.9169
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2422 Acc: 0.9276
val Loss: 0.2595 Acc: 0.9226
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2320 Acc: 0.9311
val Loss: 0.2636 Acc: 0.9214
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2250 Acc: 0.9333
val Loss: 0.2641 Acc: 0.9186
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2224 Acc: 0.9336
val Loss: 0.2517 Acc: 0.9262
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2171 Acc: 0.9348
val Loss: 0.2468 Acc: 0.9280
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2171 Acc: 0.9336
val Loss: 0.2602 Acc: 0.9219
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2160 Acc: 0.9361
val Loss: 0.2605 Acc: 0.9251
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2129 Acc: 0.9369
val Loss: 0.2541 Acc: 0.9253
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2099 Acc: 0.9369
val Loss: 0.2590 Acc: 0.9239
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2105 Acc: 0.9372
val Loss: 0.2455 Acc: 0.9276
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2085 Acc: 0.9375
val Loss: 0.2644 Acc: 0.9224
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2099 Acc: 0.9379
val Loss: 0.2615 Acc: 0.9225
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2079 Acc: 0.9382
val Loss: 0.2308 Acc: 0.9313
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1801 Acc: 0.9475
val Loss: 0.2198 Acc: 0.9362
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1706 Acc: 0.9508
val Loss: 0.2106 Acc: 0.9381
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1662 Acc: 0.9516
val Loss: 0.2108 Acc: 0.9381
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1644 Acc: 0.9531
val Loss: 0.2102 Acc: 0.9396
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1657 Acc: 0.9525
val Loss: 0.2063 Acc: 0.9407
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1646 Acc: 0.9533
val Loss: 0.2097 Acc: 0.9382
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1634 Acc: 0.9524
val Loss: 0.2084 Acc: 0.9394
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1626 Acc: 0.9533
val Loss: 0.2105 Acc: 0.9398
Epoch finished in 0m 9s
Best validation accuracy: 0.9397728513705362
Model Test Accuracy:  0.945759065765212
Pruning Epoch 8
++++++++++++++++++
number of weights to prune:  11227.0
Sparsity of Pruned Mask:  tensor(0.8322)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.6458 Acc: 0.5025
val Loss: 0.8327 Acc: 0.8263
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5091 Acc: 0.8774
val Loss: 0.3678 Acc: 0.8985
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3122 Acc: 0.9106
val Loss: 0.2953 Acc: 0.9138
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2622 Acc: 0.9235
val Loss: 0.2732 Acc: 0.9189
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2422 Acc: 0.9278
val Loss: 0.2687 Acc: 0.9189
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2310 Acc: 0.9311
val Loss: 0.2625 Acc: 0.9215
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2217 Acc: 0.9338
val Loss: 0.2600 Acc: 0.9230
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2230 Acc: 0.9329
val Loss: 0.2502 Acc: 0.9255
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2170 Acc: 0.9365
val Loss: 0.2500 Acc: 0.9246
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2185 Acc: 0.9339
val Loss: 0.2616 Acc: 0.9225
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2156 Acc: 0.9361
val Loss: 0.2560 Acc: 0.9242
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2142 Acc: 0.9351
val Loss: 0.2439 Acc: 0.9279
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2128 Acc: 0.9358
val Loss: 0.2842 Acc: 0.9162
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2092 Acc: 0.9379
val Loss: 0.2685 Acc: 0.9201
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2099 Acc: 0.9368
val Loss: 0.2955 Acc: 0.9126
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2128 Acc: 0.9364
val Loss: 0.2709 Acc: 0.9213
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2072 Acc: 0.9378
val Loss: 0.2280 Acc: 0.9326
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1816 Acc: 0.9466
val Loss: 0.2197 Acc: 0.9372
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1740 Acc: 0.9501
val Loss: 0.2157 Acc: 0.9379
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1692 Acc: 0.9503
val Loss: 0.2132 Acc: 0.9383
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1685 Acc: 0.9513
val Loss: 0.2135 Acc: 0.9376
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1681 Acc: 0.9514
val Loss: 0.2127 Acc: 0.9373
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1679 Acc: 0.9513
val Loss: 0.2145 Acc: 0.9367
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1679 Acc: 0.9515
val Loss: 0.2103 Acc: 0.9392
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1674 Acc: 0.9516
val Loss: 0.2127 Acc: 0.9376
Epoch finished in 0m 9s
Best validation accuracy: 0.9375887299333843
Model Test Accuracy:  0.9474108789182544
Pruning Epoch 9
++++++++++++++++++
number of weights to prune:  8982.0
Sparsity of Pruned Mask:  tensor(0.8658)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.6183 Acc: 0.5270
val Loss: 0.7982 Acc: 0.8373
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5032 Acc: 0.8786
val Loss: 0.3544 Acc: 0.9036
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3122 Acc: 0.9106
val Loss: 0.2990 Acc: 0.9097
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2646 Acc: 0.9219
val Loss: 0.2821 Acc: 0.9151
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2432 Acc: 0.9282
val Loss: 0.2649 Acc: 0.9200
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2324 Acc: 0.9313
val Loss: 0.2642 Acc: 0.9218
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2261 Acc: 0.9328
val Loss: 0.2508 Acc: 0.9263
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2231 Acc: 0.9333
val Loss: 0.2652 Acc: 0.9201
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2204 Acc: 0.9343
val Loss: 0.2555 Acc: 0.9240
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2197 Acc: 0.9334
val Loss: 0.2591 Acc: 0.9228
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2168 Acc: 0.9351
val Loss: 0.2521 Acc: 0.9260
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2178 Acc: 0.9344
val Loss: 0.2513 Acc: 0.9252
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2148 Acc: 0.9361
val Loss: 0.2647 Acc: 0.9223
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2174 Acc: 0.9351
val Loss: 0.2614 Acc: 0.9212
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2135 Acc: 0.9358
val Loss: 0.2611 Acc: 0.9224
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2140 Acc: 0.9353
val Loss: 0.2553 Acc: 0.9250
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2082 Acc: 0.9386
val Loss: 0.2316 Acc: 0.9326
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1864 Acc: 0.9453
val Loss: 0.2223 Acc: 0.9372
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1795 Acc: 0.9480
val Loss: 0.2124 Acc: 0.9380
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1764 Acc: 0.9486
val Loss: 0.2112 Acc: 0.9387
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1745 Acc: 0.9503
val Loss: 0.2135 Acc: 0.9375
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1713 Acc: 0.9509
val Loss: 0.2124 Acc: 0.9382
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1709 Acc: 0.9507
val Loss: 0.2088 Acc: 0.9409
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1713 Acc: 0.9507
val Loss: 0.2086 Acc: 0.9405
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1738 Acc: 0.9511
val Loss: 0.2114 Acc: 0.9391
Epoch finished in 0m 9s
Best validation accuracy: 0.9390630119034619
Model Test Accuracy:  0.9454517516902273
Pruning Epoch 10
++++++++++++++++++
number of weights to prune:  7185.0
Sparsity of Pruned Mask:  tensor(0.8926)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.6906 Acc: 0.4898
val Loss: 0.8626 Acc: 0.8147
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5382 Acc: 0.8689
val Loss: 0.3741 Acc: 0.8976
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3230 Acc: 0.9085
val Loss: 0.2896 Acc: 0.9167
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2679 Acc: 0.9214
val Loss: 0.2722 Acc: 0.9185
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2491 Acc: 0.9259
val Loss: 0.2619 Acc: 0.9208
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2354 Acc: 0.9300
val Loss: 0.2694 Acc: 0.9203
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2312 Acc: 0.9311
val Loss: 0.2549 Acc: 0.9250
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2288 Acc: 0.9317
val Loss: 0.2716 Acc: 0.9178
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2267 Acc: 0.9318
val Loss: 0.2684 Acc: 0.9182
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2262 Acc: 0.9321
val Loss: 0.2667 Acc: 0.9215
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2192 Acc: 0.9338
val Loss: 0.2525 Acc: 0.9265
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2221 Acc: 0.9333
val Loss: 0.2632 Acc: 0.9216
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2215 Acc: 0.9337
val Loss: 0.2808 Acc: 0.9174
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2195 Acc: 0.9333
val Loss: 0.2513 Acc: 0.9248
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2189 Acc: 0.9356
val Loss: 0.2786 Acc: 0.9145
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2217 Acc: 0.9333
val Loss: 0.2971 Acc: 0.9119
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2221 Acc: 0.9330
val Loss: 0.2393 Acc: 0.9291
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.1931 Acc: 0.9424
val Loss: 0.2206 Acc: 0.9360
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1833 Acc: 0.9464
val Loss: 0.2220 Acc: 0.9343
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1812 Acc: 0.9474
val Loss: 0.2160 Acc: 0.9369
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1792 Acc: 0.9476
val Loss: 0.2171 Acc: 0.9371
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1775 Acc: 0.9480
val Loss: 0.2154 Acc: 0.9396
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1776 Acc: 0.9487
val Loss: 0.2195 Acc: 0.9373
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1772 Acc: 0.9483
val Loss: 0.2122 Acc: 0.9403
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1767 Acc: 0.9480
val Loss: 0.2172 Acc: 0.9360
Epoch finished in 0m 9s
Best validation accuracy: 0.9359506388555204
Model Test Accuracy:  0.9434926244622003
Pruning Epoch 11
++++++++++++++++++
number of weights to prune:  5748.0
Sparsity of Pruned Mask:  tensor(0.9141)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.7484 Acc: 0.4553
val Loss: 0.9423 Acc: 0.7949
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.5741 Acc: 0.8633
val Loss: 0.3938 Acc: 0.8944
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3335 Acc: 0.9054
val Loss: 0.3141 Acc: 0.9092
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2738 Acc: 0.9196
val Loss: 0.2884 Acc: 0.9126
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2532 Acc: 0.9241
val Loss: 0.2617 Acc: 0.9225
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2435 Acc: 0.9273
val Loss: 0.2753 Acc: 0.9173
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2368 Acc: 0.9294
val Loss: 0.2751 Acc: 0.9172
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2353 Acc: 0.9285
val Loss: 0.2610 Acc: 0.9216
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2325 Acc: 0.9299
val Loss: 0.2643 Acc: 0.9215
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2311 Acc: 0.9311
val Loss: 0.2772 Acc: 0.9182
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2308 Acc: 0.9306
val Loss: 0.2544 Acc: 0.9247
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2278 Acc: 0.9312
val Loss: 0.2532 Acc: 0.9245
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2273 Acc: 0.9320
val Loss: 0.2603 Acc: 0.9241
Epoch finished in 0m 9s
Training Epoch 13/24
********************
Warmup
train Loss: 0.2256 Acc: 0.9329
val Loss: 0.2778 Acc: 0.9184
Epoch finished in 0m 9s
Training Epoch 14/24
********************
Warmup
train Loss: 0.2248 Acc: 0.9331
val Loss: 0.2801 Acc: 0.9177
Epoch finished in 0m 9s
Training Epoch 15/24
********************
Warmup
train Loss: 0.2275 Acc: 0.9317
val Loss: 0.3103 Acc: 0.9064
Epoch finished in 0m 9s
Training Epoch 16/24
********************
Warmup
train Loss: 0.2272 Acc: 0.9319
val Loss: 0.2420 Acc: 0.9269
Epoch finished in 0m 9s
Training Epoch 17/24
********************
Milestone 2: 0.01
train Loss: 0.2068 Acc: 0.9385
val Loss: 0.2293 Acc: 0.9323
Epoch finished in 0m 9s
Training Epoch 18/24
********************
Milestone 2: 0.01
train Loss: 0.1920 Acc: 0.9444
val Loss: 0.2205 Acc: 0.9351
Epoch finished in 0m 9s
Training Epoch 19/24
********************
Milestone 2: 0.01
train Loss: 0.1895 Acc: 0.9454
val Loss: 0.2180 Acc: 0.9349
Epoch finished in 0m 9s
Training Epoch 20/24
********************
Milestone 3: 0.001
train Loss: 0.1882 Acc: 0.9455
val Loss: 0.2187 Acc: 0.9363
Epoch finished in 0m 9s
Training Epoch 21/24
********************
Milestone 3: 0.001
train Loss: 0.1874 Acc: 0.9453
val Loss: 0.2144 Acc: 0.9387
Epoch finished in 0m 9s
Training Epoch 22/24
********************
Milestone 3: 0.001
train Loss: 0.1855 Acc: 0.9456
val Loss: 0.2190 Acc: 0.9379
Epoch finished in 0m 9s
Training Epoch 23/24
********************
Milestone 3: 0.001
train Loss: 0.1846 Acc: 0.9459
val Loss: 0.2222 Acc: 0.9355
Epoch finished in 0m 9s
Training Epoch 24/24
********************
Milestone 3: 0.001
train Loss: 0.1870 Acc: 0.9464
val Loss: 0.2186 Acc: 0.9368
Epoch finished in 0m 9s
Best validation accuracy: 0.9367696843944523
Model Test Accuracy:  0.9413414259373079
Pruning Epoch 12
++++++++++++++++++
number of weights to prune:  4598.0
Sparsity of Pruned Mask:  tensor(0.9313)
[10, 15, 20]
Warmup
Training Epoch 0/24
********************
train Loss: 1.8257 Acc: 0.4179
val Loss: 1.0442 Acc: 0.7688
Epoch finished in 0m 9s
Training Epoch 1/24
********************
Warmup
train Loss: 0.6243 Acc: 0.8505
val Loss: 0.4134 Acc: 0.8897
Epoch finished in 0m 9s
Training Epoch 2/24
********************
Warmup
train Loss: 0.3472 Acc: 0.9035
val Loss: 0.3192 Acc: 0.9102
Epoch finished in 0m 9s
Training Epoch 3/24
********************
Warmup
train Loss: 0.2857 Acc: 0.9166
val Loss: 0.2898 Acc: 0.9137
Epoch finished in 0m 9s
Training Epoch 4/24
********************
Warmup
train Loss: 0.2609 Acc: 0.9226
val Loss: 0.2718 Acc: 0.9204
Epoch finished in 0m 9s
Training Epoch 5/24
********************
Warmup
train Loss: 0.2501 Acc: 0.9253
val Loss: 0.2887 Acc: 0.9147
Epoch finished in 0m 9s
Training Epoch 6/24
********************
Warmup
train Loss: 0.2445 Acc: 0.9264
val Loss: 0.2830 Acc: 0.9138
Epoch finished in 0m 9s
Training Epoch 7/24
********************
Warmup
train Loss: 0.2469 Acc: 0.9254
val Loss: 0.2708 Acc: 0.9194
Epoch finished in 0m 9s
Training Epoch 8/24
********************
Warmup
train Loss: 0.2400 Acc: 0.9275
val Loss: 0.2927 Acc: 0.9085
Epoch finished in 0m 9s
Training Epoch 9/24
********************
Warmup
train Loss: 0.2392 Acc: 0.9282
val Loss: 0.2733 Acc: 0.9175
Epoch finished in 0m 9s
Training Epoch 10/24
********************
Warmup
train Loss: 0.2380 Acc: 0.9294
val Loss: 0.2697 Acc: 0.9186
Epoch finished in 0m 9s
Training Epoch 11/24
********************
Warmup
train Loss: 0.2379 Acc: 0.9283
val Loss: 0.2841 Acc: 0.9145
Epoch finished in 0m 9s
Training Epoch 12/24
********************
Warmup
train Loss: 0.2377 Acc: 0.9287
val Loss: 0.2722 Acc: 0.9201
Epoch finished in 0m 9s
Training Epoch 13/24
********************
