Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f15809e4690
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f15809f3820
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f158ddecf50
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f158017c140
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f158017c5f0
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f158017ca50
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f158018a500
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f158018ae10
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801994b0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801995f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f1580199500
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f158018aeb0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f158018af50
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f1580199050
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f1580199370
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f1580199780
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801998c0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2910
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2460
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2280
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2be0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2410
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2500
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2a00
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2aa0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2f50
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801a2d20
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801b08c0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801b0e60
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801ba280
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801ba3c0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801b0910
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801b09b0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801b0aa0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801ba050
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f15801ba140
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2649 Acc: 0.1693
val Loss: 2.2322 Acc: 0.1888
Epoch finished in 0m 5s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2171 Acc: 0.2065
val Loss: 2.1748 Acc: 0.2509
Epoch finished in 0m 5s
Best validation accuracy: 0.25090095009282515
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.27412415488629377
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2547 Acc: 0.1753
val Loss: 2.2167 Acc: 0.1961
Epoch finished in 0m 5s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1894 Acc: 0.2272
val Loss: 2.1439 Acc: 0.2595
Epoch finished in 0m 5s
Best validation accuracy: 0.2595282297695752
Model Test Accuracy:  0.2763521819299324
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3438 Acc: 0.1631
val Loss: 2.2341 Acc: 0.1887
Epoch finished in 0m 7s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2326 Acc: 0.1890
val Loss: 2.2247 Acc: 0.1885
Epoch finished in 0m 7s
Best validation accuracy: 0.18854428306213825
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.1952212661339889
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3395 Acc: 0.1704
val Loss: 2.2324 Acc: 0.1888
Epoch finished in 0m 7s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2308 Acc: 0.1893
val Loss: 2.2239 Acc: 0.1887
Epoch finished in 0m 7s
Best validation accuracy: 0.18870809216992465
Model Test Accuracy:  0.19549016594960048
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
train Loss: 2.2804 Acc: 0.1669
val Loss: 2.2278 Acc: 0.1929
Epoch finished in 0m 23s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1545 Acc: 0.2393
val Loss: 2.0680 Acc: 0.2695
Epoch finished in 0m 22s
Best validation accuracy: 0.26952058534454515
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.28714658881376764
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2603 Acc: 0.1825
val Loss: 2.1984 Acc: 0.2085
Epoch finished in 0m 22s
Training Epoch 1/1
********************
Warmup
train Loss: 2.0637 Acc: 0.2679
val Loss: 2.0157 Acc: 0.2856
Epoch finished in 0m 22s
Best validation accuracy: 0.2856284809435405
Model Test Accuracy:  0.3102719729563614
5
conv_layer_1.weight
x:  16
y:  3
layer1.0.conv_layer_1.weight
x:  16
y:  16
layer1.0.conv_layer_2.weight
x:  16
y:  16
layer1.1.conv_layer_1.weight
x:  16
y:  16
layer1.1.conv_layer_2.weight
x:  16
y:  16
layer1.2.conv_layer_1.weight
x:  16
y:  16
layer1.2.conv_layer_2.weight
x:  16
y:  16
conv_layer_1.weight
48.604651162790695
x:  16
y:  3
layer1.0.conv_layer_1.weight
48.39270199826238
x:  16
y:  16
layer1.0.conv_layer_2.weight
50.43440486533449
x:  16
y:  16
layer1.1.conv_layer_1.weight
51.08601216333623
x:  16
y:  16
layer1.1.conv_layer_2.weight
50.30408340573415
x:  16
y:  16
layer1.2.conv_layer_1.weight
49.348392701998264
x:  16
y:  16
layer1.2.conv_layer_2.weight
50.347523892267596
x:  16
y:  16
conv_layer_1.weight
51.395348837209305
x:  16
y:  3
layer1.0.conv_layer_1.weight
48.95742832319722
x:  16
y:  16
layer1.0.conv_layer_2.weight
48.95742832319722
x:  16
y:  16
layer1.1.conv_layer_1.weight
50.47784535186794
x:  16
y:  16
layer1.1.conv_layer_2.weight
50.130321459600346
x:  16
y:  16
layer1.2.conv_layer_1.weight
50.78192875760209
x:  16
y:  16
layer1.2.conv_layer_2.weight
50.0868809730669
x:  16
y:  16
conv_layer_1.weight
51.627906976744185
x:  16
y:  3
layer1.0.conv_layer_1.weight
50.0
x:  16
y:  16
layer1.0.conv_layer_2.weight
51.216333622936574
x:  16
y:  16
layer1.1.conv_layer_1.weight
49.826238053866206
x:  16
y:  16
layer1.1.conv_layer_2.weight
49.9131190269331
x:  16
y:  16
layer1.2.conv_layer_1.weight
48.74022589052998
x:  16
y:  16
layer1.2.conv_layer_2.weight
49.652476107732404
x:  16
y:  16
                        Layer 0      Layer 1  ...                       
                                BasicBlock 0  ... BasicBlock 2          
                        conv. 0      conv. 0  ...      conv. 0   conv. 1
Original Model          [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
ReLU ResNet8            [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
univ. rational ResNet8  [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
mix. exp. ResNet8       [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]

[4 rows x 7 columns]
                       Layer 0      Layer 1  ...                     
                               BasicBlock 0  ... BasicBlock 2        
                       conv. 0      conv. 0  ...      conv. 0 conv. 1
Original Model             430         2302  ...         2302    2302
ReLU ResNet8               209         1114  ...         1136    1159
univ. rational ResNet8     221         1127  ...         1169    1153
mix. exp. ResNet8          222         1151  ...         1122    1143

[4 rows x 7 columns]
                          Layer 0      Layer 1  ...                        
                                  BasicBlock 0  ... BasicBlock 2           
                          conv. 0      conv. 0  ...      conv. 0    conv. 1
ReLU ResNet8            48.604651    48.392702  ...    49.348393  50.347524
univ. rational ResNet8  51.395349    48.957428  ...    50.781929  50.086881
mix. exp. ResNet8       51.627907    50.000000  ...    48.740226  49.652476

[3 rows x 7 columns]
