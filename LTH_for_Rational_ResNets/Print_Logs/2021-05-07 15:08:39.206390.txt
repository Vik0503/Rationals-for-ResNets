Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fc79044e780
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fc79045daa0
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fc79045df00
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fc790465410
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fc790465870
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fc790465cd0
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc7901f45a0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc7902012d0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201320
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201230
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201aa0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201280
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc7902011e0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201140
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79df40320
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201960
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790201cd0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc793289b40
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020c730
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020c7d0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020c6e0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020c690
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020c550
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020cbe0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc79020c500
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790432a50
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790219460
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790219370
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790219a50
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc7902250f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790225280
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790219b40
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790219be0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790225370
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc790225140
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fc7902255f0
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3037 Acc: 0.1722
val Loss: 2.2309 Acc: 0.1919
Epoch finished in 0m 5s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2176 Acc: 0.1977
val Loss: 2.1837 Acc: 0.2275
Epoch finished in 0m 5s
Best validation accuracy: 0.22747624767937097
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.24519821757836507
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2968 Acc: 0.1811
val Loss: 2.2203 Acc: 0.1935
Epoch finished in 0m 5s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1951 Acc: 0.2146
val Loss: 2.1448 Acc: 0.2457
Epoch finished in 0m 5s
Best validation accuracy: 0.24565905864366058
Model Test Accuracy:  0.2708973570989551
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3042 Acc: 0.1563
val Loss: 2.2351 Acc: 0.1902
Epoch finished in 0m 7s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2273 Acc: 0.1897
val Loss: 2.2089 Acc: 0.2050
Epoch finished in 0m 7s
Best validation accuracy: 0.20497979687670634
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.2230716041794714
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2960 Acc: 0.1611
val Loss: 2.2289 Acc: 0.1920
Epoch finished in 0m 7s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2156 Acc: 0.2000
val Loss: 2.1875 Acc: 0.2350
Epoch finished in 0m 7s
Best validation accuracy: 0.23501146663754505
Model Test Accuracy:  0.2625230485556238
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
train Loss: 2.2563 Acc: 0.1816
val Loss: 2.2324 Acc: 0.1914
Epoch finished in 0m 22s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2270 Acc: 0.1920
val Loss: 2.2035 Acc: 0.2258
Epoch finished in 0m 22s
Best validation accuracy: 0.22578355356557825
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.23824523663183772
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2504 Acc: 0.1874
val Loss: 2.2256 Acc: 0.1919
Epoch finished in 0m 22s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2137 Acc: 0.2020
val Loss: 2.1898 Acc: 0.2285
Epoch finished in 0m 23s
Best validation accuracy: 0.22851370536201812
Model Test Accuracy:  0.23732329440688382
conv_layer_1.weight
x:  16
y:  3
layer1.0.conv_layer_1.weight
x:  16
y:  16
layer1.0.conv_layer_2.weight
x:  16
y:  16
layer1.1.conv_layer_1.weight
x:  16
y:  16
layer1.1.conv_layer_2.weight
x:  16
y:  16
layer1.2.conv_layer_1.weight
x:  16
y:  16
layer1.2.conv_layer_2.weight
x:  16
y:  16
conv_layer_1.weight
46.27906976744186
x:  16
y:  3
layer1.0.conv_layer_1.weight
49.95655951346655
x:  16
y:  16
layer1.0.conv_layer_2.weight
50.0
x:  16
y:  16
layer1.1.conv_layer_1.weight
49.9131190269331
x:  16
y:  16
layer1.1.conv_layer_2.weight
51.390095569070375
x:  16
y:  16
layer1.2.conv_layer_1.weight
50.130321459600346
x:  16
y:  16
layer1.2.conv_layer_2.weight
48.95742832319722
x:  16
y:  16
conv_layer_1.weight
51.395348837209305
x:  16
y:  3
layer1.0.conv_layer_1.weight
49.869678540399654
x:  16
y:  16
layer1.0.conv_layer_2.weight
50.04344048653345
x:  16
y:  16
layer1.1.conv_layer_1.weight
50.0868809730669
x:  16
y:  16
layer1.1.conv_layer_2.weight
49.826238053866206
x:  16
y:  16
layer1.2.conv_layer_1.weight
49.131190269331015
x:  16
y:  16
layer1.2.conv_layer_2.weight
50.43440486533449
x:  16
y:  16
conv_layer_1.weight
52.7906976744186
x:  16
y:  3
layer1.0.conv_layer_1.weight
49.9131190269331
x:  16
y:  16
layer1.0.conv_layer_2.weight
50.60816681146829
x:  16
y:  16
layer1.1.conv_layer_1.weight
47.87141615986099
x:  16
y:  16
layer1.1.conv_layer_2.weight
49.78279756733276
x:  16
y:  16
layer1.2.conv_layer_1.weight
50.60816681146829
x:  16
y:  16
layer1.2.conv_layer_2.weight
50.347523892267596
x:  16
y:  16
                        Layer 0      Layer 1  ...                       
                                BasicBlock 0  ... BasicBlock 2          
                        conv. 0      conv. 0  ...      conv. 0   conv. 1
Original Model          [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
ReLU ResNet8            [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
univ. rational ResNet8  [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
mix. exp. ResNet8       [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]

[4 rows x 7 columns]
                       Layer 0      Layer 1  ...                     
                               BasicBlock 0  ... BasicBlock 2        
                       conv. 0      conv. 0  ...      conv. 0 conv. 1
Original Model             430         2302  ...         2302    2302
ReLU ResNet8               199         1150  ...         1154    1127
univ. rational ResNet8     221         1148  ...         1131    1161
mix. exp. ResNet8          227         1149  ...         1165    1159

[4 rows x 7 columns]
                          Layer 0      Layer 1  ...                        
                                  BasicBlock 0  ... BasicBlock 2           
                          conv. 0      conv. 0  ...      conv. 0    conv. 1
ReLU ResNet8            46.279070    49.956560  ...    50.130321  48.957428
univ. rational ResNet8  51.395349    49.869679  ...    49.131190  50.434405
mix. exp. ResNet8       52.790698    49.913119  ...    50.608167  50.347524

[3 rows x 7 columns]
