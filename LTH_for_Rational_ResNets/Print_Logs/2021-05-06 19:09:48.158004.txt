Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbb45f0
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbc18c0
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbc1d20
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbc91e0
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbc9640
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbc9a50
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbc9eb0
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbd7410
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbd7870
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940cbd7eb0
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c164460
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c164820
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c164c80
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c16e1e0
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c16e640
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c16ec80
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c17c230
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f940c17c640
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c18a140
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c18a0f0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c18aa50
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1953c0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195410
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c18ab40
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c18ac30
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c18abe0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195190
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195140
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195280
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195eb0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1200a0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1206e0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1205f0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195690
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1205a0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c120190
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1200f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c120870
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c195e60
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12b5f0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12b6e0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12bb90
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12bd20
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12b5a0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12b4b0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12b460
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12bcd0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c12b640
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c138320
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c138230
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c138cd0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c144370
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c144280
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c138e10
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c138dc0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c138910
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1440a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1444b0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c144230
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153230
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153280
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153190
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153960
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1531e0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153140
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1530a0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153820
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153aa0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c153d20
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15d870
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15d8c0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15dd70
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e83c0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15d820
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15d780
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15de60
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c15d7d0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e8190
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e8410
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e8280
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e8d70
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0f6690
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0f66e0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e8dc0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f9421a1ef00
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0e8e10
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0f6370
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0f64b0
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0f6320
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ff550
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ff5a0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ff4b0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ffdc0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ff500
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ff460
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ff3c0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c0ffc80
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c10c2d0
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c10c050
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c10ca00
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c10c410
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c118050
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c118410
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c10caa0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c10cb40
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1182d0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c1181e0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f940c118190
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
train Loss: 2.2606 Acc: 0.1814
val Loss: 2.2223 Acc: 0.1935
Epoch finished in 0m 9s
Training Epoch 1/2
********************
Warmup
train Loss: 2.0719 Acc: 0.2594
val Loss: 1.8934 Acc: 0.3103
Epoch finished in 0m 9s
Training Epoch 2/2
********************
Warmup
train Loss: 1.6684 Acc: 0.4057
val Loss: 1.5115 Acc: 0.4671
Epoch finished in 0m 9s
Best validation accuracy: 0.467074369334935
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.46139366933005527
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  54052.0
Sparsity of Pruned Mask:  tensor(0.2000)
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
train Loss: 2.1731 Acc: 0.2283
val Loss: 1.9942 Acc: 0.2851
Epoch finished in 0m 9s
Training Epoch 1/2
********************
Warmup
train Loss: 1.7564 Acc: 0.3754
val Loss: 1.5459 Acc: 0.4506
Epoch finished in 0m 9s
Training Epoch 2/2
********************
Warmup
train Loss: 1.1825 Acc: 0.5975
val Loss: 0.9510 Acc: 0.6823
Epoch finished in 0m 9s
Best validation accuracy: 0.6823195369662554
Model Test Accuracy:  0.6690611555009219
Pruning Epoch 2
++++++++++++++++++
number of weights to prune:  43241.0
Sparsity of Pruned Mask:  tensor(0.3600)
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
train Loss: 2.0739 Acc: 0.2651
val Loss: 1.7570 Acc: 0.3737
Epoch finished in 0m 9s
Training Epoch 1/2
********************
Warmup
train Loss: 1.3289 Acc: 0.5593
val Loss: 1.1372 Acc: 0.6137
Epoch finished in 0m 9s
Training Epoch 2/2
********************
Warmup
train Loss: 0.8090 Acc: 0.7358
val Loss: 0.7292 Acc: 0.7671
Epoch finished in 0m 10s
Best validation accuracy: 0.7671180517636781
Model Test Accuracy:  0.771857713583282
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
train Loss: 2.2720 Acc: 0.1754
val Loss: 2.2302 Acc: 0.1920
Epoch finished in 0m 12s
Training Epoch 1/2
********************
Warmup
train Loss: 2.0786 Acc: 0.2593
val Loss: 1.8517 Acc: 0.3319
Epoch finished in 0m 13s
Training Epoch 2/2
********************
Warmup
train Loss: 1.2833 Acc: 0.5605
val Loss: 1.0935 Acc: 0.6311
Epoch finished in 0m 12s
Best validation accuracy: 0.6311018892650432
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.6054087277197295
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  54052.0
Sparsity of Pruned Mask:  tensor(0.2000)
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
train Loss: 2.2249 Acc: 0.1996
val Loss: 2.1038 Acc: 0.2695
Epoch finished in 0m 12s
Training Epoch 1/2
********************
Warmup
train Loss: 1.4231 Acc: 0.5073
val Loss: 0.7986 Acc: 0.7469
Epoch finished in 0m 12s
Training Epoch 2/2
********************
Warmup
train Loss: 0.6190 Acc: 0.8044
val Loss: 0.6150 Acc: 0.8057
Epoch finished in 0m 12s
Best validation accuracy: 0.805722398165338
Model Test Accuracy:  0.8029348494161032
Pruning Epoch 2
++++++++++++++++++
number of weights to prune:  43241.0
Sparsity of Pruned Mask:  tensor(0.3600)
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
train Loss: 2.1245 Acc: 0.2428
val Loss: 1.6826 Acc: 0.4191
Epoch finished in 0m 12s
Training Epoch 1/2
********************
Warmup
train Loss: 0.8325 Acc: 0.7292
val Loss: 0.5942 Acc: 0.8130
Epoch finished in 0m 13s
Training Epoch 2/2
********************
Warmup
train Loss: 0.5108 Acc: 0.8409
val Loss: 0.5797 Acc: 0.8419
Epoch finished in 0m 13s
Best validation accuracy: 0.841869607950202
Model Test Accuracy:  0.8370467117393976
[10, 15, 20]
Warmup
Training Epoch 0/2
********************
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
train Loss: 2.2550 Acc: 0.1731
val Loss: 2.2363 Acc: 0.1880
Epoch finished in 0m 43s
Training Epoch 1/2
********************
Warmup
train Loss: 2.1187 Acc: 0.2396
val Loss: 1.7432 Acc: 0.3826
Epoch finished in 0m 42s
Training Epoch 2/2
********************
Warmup
train Loss: 1.0747 Acc: 0.6396
val Loss: 0.7926 Acc: 0.7683
Epoch finished in 0m 42s
Best validation accuracy: 0.7682647155181829
