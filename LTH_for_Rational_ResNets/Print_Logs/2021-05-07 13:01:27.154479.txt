Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f63135f0
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f631f8c0
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f631fcd0
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f63271e0
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6327640
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6327aa0
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6327f00
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6334410
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6334870
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6334e60
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50c2410
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50c2820
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f6313be0
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50cd190
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50cd5f0
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50cdc30
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50d91e0
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7f43f50d95f0
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50e80f0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50e80a0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50e8f00
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50e89b0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f31e0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50e8aa0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f441378df00
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50e8be0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f3410
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f3460
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f30a0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f35a0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f3dc0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50800a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5080820
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f3eb0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50f3f50
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50806e0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50805f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50805a0
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5080730
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088550
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088460
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50884b0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50883c0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088320
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088410
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088370
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50889b0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088d20
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5088e60
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5096140
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5096be0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50a3320
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50a3230
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5096d20
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5096cd0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5096820
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50a30a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50a3460
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50a31e0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b11e0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1230
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1140
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1910
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1190
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b10f0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1050
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b17d0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1a50
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50b1cd0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503b820
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503b870
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503bd20
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5046370
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503b7d0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503b730
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503b690
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f503be60
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50460f0
    )
    (conv_layer_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5046280
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5046230
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5046c30
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50532d0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5053690
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5046cd0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5046dc0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5053550
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5053460
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5053410
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50535a0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505c550
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505c5a0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505c4b0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505cdc0
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505c500
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505c460
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505c3c0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f505cc80
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50692d0
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5069050
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5069410
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5069a00
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50750a0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5075460
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5069af0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5069b90
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5075320
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f5075230
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7f43f50751e0
    )
    (conv_layer_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2496 Acc: 0.1827
val Loss: 2.2294 Acc: 0.1934
Epoch finished in 0m 9s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1418 Acc: 0.2352
val Loss: 1.9738 Acc: 0.2955
Epoch finished in 0m 9s
Best validation accuracy: 0.29551163044665285
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.2816533497234173
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  135128.0
Sparsity of Pruned Mask:  tensor(0.5000)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.1699 Acc: 0.2308
val Loss: 2.0151 Acc: 0.2803
Epoch finished in 0m 9s
Training Epoch 1/1
********************
Warmup
train Loss: 1.9148 Acc: 0.3145
val Loss: 1.7643 Acc: 0.3826
Epoch finished in 0m 9s
Best validation accuracy: 0.3826034727530851
Model Test Accuracy:  0.37922556853103867
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2495 Acc: 0.1729
val Loss: 2.2308 Acc: 0.1944
Epoch finished in 0m 12s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1211 Acc: 0.2406
val Loss: 1.8972 Acc: 0.3170
Epoch finished in 0m 12s
Best validation accuracy: 0.31697062356667033
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.32556084818684694
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  135128.0
Sparsity of Pruned Mask:  tensor(0.5000)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.1514 Acc: 0.2278
val Loss: 2.0054 Acc: 0.2815
Epoch finished in 0m 12s
Training Epoch 1/1
********************
Warmup
train Loss: 1.7699 Acc: 0.3772
val Loss: 1.4353 Acc: 0.5068
Epoch finished in 0m 12s
Best validation accuracy: 0.5067707764551709
Model Test Accuracy:  0.5052243392747388
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
train Loss: 2.2448 Acc: 0.1785
val Loss: 2.2591 Acc: 0.1952
Epoch finished in 0m 43s
Training Epoch 1/1
********************
Warmup
train Loss: 1.7308 Acc: 0.3995
val Loss: 1.6206 Acc: 0.5109
Epoch finished in 0m 43s
Best validation accuracy: 0.5109206071857595
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.5303856791641056
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  135128.0
Sparsity of Pruned Mask:  tensor(0.5000)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 1.8811 Acc: 0.3476
val Loss: 1.4694 Acc: 0.4859
Epoch finished in 0m 42s
Training Epoch 1/1
********************
Warmup
train Loss: 0.9958 Acc: 0.6720
val Loss: 0.7993 Acc: 0.7507
Epoch finished in 0m 42s
Best validation accuracy: 0.7507371409850387
Model Test Accuracy:  0.7445067609096496
5
conv_layer_1.weight
x:  16
y:  3
layer1.0.conv_layer_1.weight
x:  16
y:  16
layer1.0.conv_layer_2.weight
x:  16
y:  16
layer1.1.conv_layer_1.weight
x:  16
y:  16
layer1.1.conv_layer_2.weight
x:  16
y:  16
layer1.2.conv_layer_1.weight
x:  16
y:  16
layer1.2.conv_layer_2.weight
x:  16
y:  16
layer2.0.conv_layer_1.weight
x:  32
y:  16
layer2.0.conv_layer_2.weight
x:  32
y:  32
layer2.0.shortcut.0.weight
x:  32
y:  16
layer2.1.conv_layer_1.weight
x:  32
y:  32
layer2.1.conv_layer_2.weight
x:  32
y:  32
layer2.2.conv_layer_1.weight
x:  32
y:  32
layer2.2.conv_layer_2.weight
x:  32
y:  32
layer3.0.conv_layer_1.weight
x:  64
y:  32
layer3.0.conv_layer_2.weight
x:  64
y:  64
layer3.0.shortcut.0.weight
x:  64
y:  32
layer3.1.conv_layer_1.weight
x:  64
y:  64
layer3.1.conv_layer_2.weight
