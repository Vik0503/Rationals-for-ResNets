Sequential(
  (0): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): BasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fe70c04b7d0
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fe70c05aaf0
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fe70c05af50
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fe70c063460
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fe70c0638c0
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
    cuda:0: 0x7fe70c063cd0
    (shortcut): Sequential()
  )
)
Sequential(
  (0): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dcb550
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd6280
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd61e0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd6780
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd6190
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd6230
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe70c02fdc0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd60f0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd6640
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd68c0
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (1): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dd6a00
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de2690
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de26e0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de25f0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de2e60
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de2640
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de25a0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de2500
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706de2d20
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706def370
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
  (2): RationalBasicBlock(
    (conv_layer_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rational_expert_group_1): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706def0f0
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706def4b0
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706defaa0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dfc140
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dfc500
    )
    (rational_expert_group_2): Sequential(
      (0): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706defb90
      (1): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706defc30
      (2): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dfc3c0
      (3): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dfc2d0
      (4): Rational Activation Function (PYTORCH version A) of degrees (5, 4) running on cuda
      cuda:0: 0x7fe706dfc280
    )
    (conv_layer_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (batch_norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (shortcut): Sequential()
  )
)
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
Retrieving input from now on.
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3087 Acc: 0.1536
val Loss: 2.2349 Acc: 0.1888
Epoch finished in 0m 5s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2340 Acc: 0.1882
val Loss: 2.2239 Acc: 0.1902
Epoch finished in 0m 5s
Best validation accuracy: 0.1901823741400022
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.19645052243392747
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3026 Acc: 0.1555
val Loss: 2.2340 Acc: 0.1878
Epoch finished in 0m 5s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2327 Acc: 0.1888
val Loss: 2.2265 Acc: 0.1899
Epoch finished in 0m 5s
Best validation accuracy: 0.1899093589603582
Model Test Accuracy:  0.19706515058389673
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.3074 Acc: 0.1520
val Loss: 2.2267 Acc: 0.1905
Epoch finished in 0m 7s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1988 Acc: 0.2208
val Loss: 2.1345 Acc: 0.2589
Epoch finished in 0m 7s
Best validation accuracy: 0.2589275963743584
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.27677473878303627
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2843 Acc: 0.1676
val Loss: 2.1968 Acc: 0.2330
Epoch finished in 0m 7s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1494 Acc: 0.2491
val Loss: 2.0868 Acc: 0.2677
Epoch finished in 0m 7s
Best validation accuracy: 0.26766408212296605
Model Test Accuracy:  0.28345881991395205
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
Training mode, no longer retrieving the input.
train Loss: 2.3097 Acc: 0.1213
val Loss: 2.2458 Acc: 0.1834
Epoch finished in 0m 22s
Training Epoch 1/1
********************
Warmup
train Loss: 2.2130 Acc: 0.2039
val Loss: 2.1560 Acc: 0.2495
Epoch finished in 0m 22s
Best validation accuracy: 0.2495358741946052
Before Pruning
++++++++++++++++++
Model Test Accuracy:  0.26678703134603565
Pruning Epoch 1
++++++++++++++++++
number of weights to prune:  7128.0
Sparsity of Pruned Mask:  tensor(0.5001)
[10, 15, 20]
Warmup
Training Epoch 0/1
********************
train Loss: 2.2820 Acc: 0.1528
val Loss: 2.1925 Acc: 0.2174
Epoch finished in 0m 22s
Training Epoch 1/1
********************
Warmup
train Loss: 2.1115 Acc: 0.2520
val Loss: 2.3003 Acc: 0.2643
Epoch finished in 0m 22s
Best validation accuracy: 0.26433329693130936
Model Test Accuracy:  0.2904118008604794
5
conv_layer_1.weight
x:  16
y:  3
layer1.0.conv_layer_1.weight
x:  16
y:  16
layer1.0.conv_layer_2.weight
x:  16
y:  16
layer1.1.conv_layer_1.weight
x:  16
y:  16
layer1.1.conv_layer_2.weight
x:  16
y:  16
layer1.2.conv_layer_1.weight
x:  16
y:  16
layer1.2.conv_layer_2.weight
x:  16
y:  16
conv_layer_1.weight
51.16279069767442
x:  16
y:  3
layer1.0.conv_layer_1.weight
48.91398783666377
x:  16
y:  16
layer1.0.conv_layer_2.weight
50.347523892267596
x:  16
y:  16
layer1.1.conv_layer_1.weight
48.21894005212858
x:  16
y:  16
layer1.1.conv_layer_2.weight
50.651607298001736
x:  16
y:  16
layer1.2.conv_layer_1.weight
50.56472632493484
x:  16
y:  16
layer1.2.conv_layer_2.weight
50.73848827106863
x:  16
y:  16
conv_layer_1.weight
50.0
x:  16
y:  3
layer1.0.conv_layer_1.weight
50.21720243266724
x:  16
y:  16
layer1.0.conv_layer_2.weight
49.087749782797566
x:  16
y:  16
layer1.1.conv_layer_1.weight
49.26151172893137
x:  16
y:  16
layer1.1.conv_layer_2.weight
49.21807124239791
x:  16
y:  16
layer1.2.conv_layer_1.weight
50.78192875760209
x:  16
y:  16
layer1.2.conv_layer_2.weight
51.08601216333623
x:  16
y:  16
conv_layer_1.weight
50.69767441860465
x:  16
y:  3
layer1.0.conv_layer_1.weight
48.95742832319722
x:  16
y:  16
layer1.0.conv_layer_2.weight
48.65334491746307
x:  16
y:  16
layer1.1.conv_layer_1.weight
51.08601216333623
x:  16
y:  16
layer1.1.conv_layer_2.weight
50.868809730668985
x:  16
y:  16
layer1.2.conv_layer_1.weight
49.609035621198956
x:  16
y:  16
layer1.2.conv_layer_2.weight
50.347523892267596
x:  16
y:  16
                        Layer 0      Layer 1  ...                       
                                BasicBlock 0  ... BasicBlock 2          
                        conv. 0      conv. 0  ...      conv. 0   conv. 1
Original Model          [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
ReLU ResNet8            [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
univ. rational ResNet8  [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]
mix. exp. ResNet8       [16, 3]     [16, 16]  ...     [16, 16]  [16, 16]

[4 rows x 7 columns]
                       Layer 0      Layer 1  ...                     
                               BasicBlock 0  ... BasicBlock 2        
                       conv. 0      conv. 0  ...      conv. 0 conv. 1
Original Model             430         2302  ...         2302    2302
ReLU ResNet8               220         1126  ...         1164    1168
univ. rational ResNet8     215         1156  ...         1169    1176
mix. exp. ResNet8          218         1127  ...         1142    1159

[4 rows x 7 columns]
                          Layer 0      Layer 1  ...                        
                                  BasicBlock 0  ... BasicBlock 2           
                          conv. 0      conv. 0  ...      conv. 0    conv. 1
ReLU ResNet8            51.162791    48.913988  ...    50.564726  50.738488
univ. rational ResNet8  50.000000    50.217202  ...    50.781929  51.086012
mix. exp. ResNet8       50.697674    48.957428  ...    49.609036  50.347524

[3 rows x 7 columns]
